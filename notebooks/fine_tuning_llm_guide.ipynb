{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26f0d88f",
   "metadata": {},
   "source": [
    "# Guide Complet : Fine-tuning d'un LLM Open-Source en Local\n",
    "\n",
    "Ce notebook vous guide √† travers le processus complet de fine-tuning d'un mod√®le de langage (LLM) open-source sur votre machine locale. Nous utiliserons des techniques modernes comme LoRA (Low-Rank Adaptation) pour un entra√Ænement efficace.\n",
    "\n",
    "## üéØ Objectifs\n",
    "- Apprendre √† fine-tuner un LLM localement\n",
    "- Utiliser LoRA pour un entra√Ænement efficace en m√©moire\n",
    "- Pr√©parer et traiter des donn√©es d'entra√Ænement\n",
    "- √âvaluer et sauvegarder le mod√®le fine-tun√©\n",
    "\n",
    "## üìã Pr√©requis\n",
    "- Python 3.8+\n",
    "- GPU recommand√© (mais CPU possible)\n",
    "- Au moins 8GB de RAM (16GB+ recommand√©)\n",
    "- Espace disque suffisant pour le mod√®le (~3-7GB selon le mod√®le)\n",
    "\n",
    "## üöÄ Commen√ßons !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86f3157",
   "metadata": {},
   "source": [
    "## 1. üì¶ Installation des Biblioth√®ques N√©cessaires\n",
    "\n",
    "Nous commen√ßons par installer toutes les biblioth√®ques requises pour le fine-tuning. Cette √©tape peut prendre quelques minutes selon votre connexion internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555337d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des biblioth√®ques principales\n",
    "# D√©commentez la ligne suivante si vous n'avez pas encore install√© les d√©pendances\n",
    "# !pip install -r ../requirements.txt\n",
    "\n",
    "# Imports n√©cessaires\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"‚úÖ Biblioth√®ques import√©es avec succ√®s\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"üîß Device disponible: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ M√©moire GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dde9bd",
   "metadata": {},
   "source": [
    "## 2. ü§ñ Charger un Mod√®le LLM Open-Source\n",
    "\n",
    "Nous allons charger un mod√®le open-source adapt√© au fine-tuning. Pour cet exemple, nous utiliserons Microsoft DialoGPT, un mod√®le conversationnel relativement l√©ger et efficace.\n",
    "\n",
    "**Options de mod√®les populaires :**\n",
    "- `microsoft/DialoGPT-medium` (117M param√®tres) - Bon pour d√©buter\n",
    "- `microsoft/DialoGPT-large` (345M param√®tres) - Plus puissant\n",
    "- `facebook/opt-350m` (350M param√®tres) - Alternative int√©ressante\n",
    "- `EleutherAI/gpt-neo-125M` (125M param√®tres) - Tr√®s l√©ger pour tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bcb562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration du mod√®le\n",
    "MODEL_NAME = \"microsoft/DialoGPT-medium\"\n",
    "CACHE_DIR = \"../models/cache\"\n",
    "\n",
    "# Cr√©er le r√©pertoire de cache\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"üîÑ Chargement du mod√®le: {MODEL_NAME}\")\n",
    "\n",
    "# Charger le tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    cache_dir=CACHE_DIR,\n",
    "    padding_side='left'  # Important pour les mod√®les causaux\n",
    ")\n",
    "\n",
    "# Ajouter un token de padding si n√©cessaire\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"‚úÖ Tokenizer charg√©\")\n",
    "print(f\"üìù Taille du vocabulaire: {len(tokenizer):,} tokens\")\n",
    "print(f\"üîö Token EOS: '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"üìã Token PAD: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")\n",
    "\n",
    "# Charger le mod√®le\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "print(f\"‚úÖ Mod√®le charg√© sur {device}\")\n",
    "\n",
    "# Informations sur le mod√®le\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"üìä Param√®tres totaux: {total_params:,}\")\n",
    "print(f\"üéØ Param√®tres entra√Ænables: {trainable_params:,}\")\n",
    "print(f\"üìà Pourcentage entra√Ænable: {(trainable_params/total_params)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e235de5",
   "metadata": {},
   "source": [
    "## 3. üìä Pr√©parer les Donn√©es d'Entra√Ænement\n",
    "\n",
    "Nous allons cr√©er un jeu de donn√©es d'exemple pour le fine-tuning. Dans un cas r√©el, vous remplaceriez ceci par vos propres donn√©es.\n",
    "\n",
    "**Format des donn√©es pour la conversation :**\n",
    "```json\n",
    "{\n",
    "  \"conversations\": [\n",
    "    {\n",
    "      \"input\": \"Question de l'utilisateur\",\n",
    "      \"output\": \"R√©ponse de l'assistant\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7278ee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er des donn√©es d'exemple pour la d√©monstration\n",
    "# Dans un cas r√©el, vous chargeriez vos propres donn√©es\n",
    "\n",
    "sample_conversations = [\n",
    "    {\n",
    "        \"input\": \"Bonjour, comment allez-vous ?\",\n",
    "        \"output\": \"Bonjour ! Je vais tr√®s bien, merci de demander. Comment puis-je vous aider aujourd'hui ?\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Pouvez-vous m'expliquer ce qu'est l'intelligence artificielle ?\",\n",
    "        \"output\": \"L'intelligence artificielle (IA) est un domaine de l'informatique qui vise √† cr√©er des machines capables de r√©aliser des t√¢ches n√©cessitant normalement l'intelligence humaine, comme la compr√©hension du langage, la reconnaissance d'images, ou la prise de d√©cisions.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Quels sont les avantages du machine learning ?\",\n",
    "        \"output\": \"Le machine learning offre plusieurs avantages : automatisation des t√¢ches complexes, capacit√© √† traiter de grandes quantit√©s de donn√©es, am√©lioration continue des performances, d√©tection de patterns invisibles √† l'≈ìil humain, et personnalisation des exp√©riences utilisateur.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Comment fonctionne un r√©seau de neurones ?\",\n",
    "        \"output\": \"Un r√©seau de neurones est inspir√© du cerveau humain. Il est compos√© de neurones artificiels organis√©s en couches qui traitent l'information. Chaque connexion a un poids qui s'ajuste pendant l'entra√Ænement pour apprendre √† reconna√Ætre des patterns dans les donn√©es.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Qu'est-ce que le deep learning ?\",\n",
    "        \"output\": \"Le deep learning est une sous-branche du machine learning qui utilise des r√©seaux de neurones profonds (avec plusieurs couches cach√©es) pour apprendre des repr√©sentations complexes des donn√©es. Il excelle dans des t√¢ches comme la vision par ordinateur et le traitement du langage naturel.\"\n",
    "    },\n",
    "    # Ajout de plus d'exemples pour un entra√Ænement plus robuste\n",
    "    {\n",
    "        \"input\": \"Merci pour votre aide !\",\n",
    "        \"output\": \"Je vous en prie ! C'√©tait un plaisir de vous aider. N'h√©sitez pas si vous avez d'autres questions.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Qu'est-ce que Python ?\",\n",
    "        \"output\": \"Python est un langage de programmation populaire, connu pour sa simplicit√© et sa lisibilit√©. Il est largement utilis√© en data science, intelligence artificielle, d√©veloppement web et automatisation.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Comment apprendre la programmation ?\",\n",
    "        \"output\": \"Pour apprendre la programmation, commencez par choisir un langage (Python est excellent pour d√©buter), pratiquez r√©guli√®rement avec des petits projets, utilisez des ressources en ligne, et n'h√©sitez pas √† rejoindre des communaut√©s de d√©veloppeurs.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"üìö Dataset cr√©√© avec {len(sample_conversations)} exemples\")\n",
    "\n",
    "# Afficher quelques exemples\n",
    "print(\"\\nüîç Aper√ßu des donn√©es:\")\n",
    "for i, conv in enumerate(sample_conversations[:3]):\n",
    "    print(f\"\\nExemple {i+1}:\")\n",
    "    print(f\"  Input: {conv['input']}\")\n",
    "    print(f\"  Output: {conv['output'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376e5cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour formater les conversations\n",
    "def format_conversation(example):\n",
    "    \"\"\"Formate une conversation pour l'entra√Ænement\"\"\"\n",
    "    input_text = example[\"input\"]\n",
    "    output_text = example[\"output\"]\n",
    "    \n",
    "    # Format conversationnel avec tokens sp√©ciaux\n",
    "    formatted_text = f\"<|user|>{input_text}<|endoftext|><|assistant|>{output_text}<|endoftext|>\"\n",
    "    return {\"text\": formatted_text}\n",
    "\n",
    "# Convertir en Dataset HuggingFace\n",
    "dataset = Dataset.from_list(sample_conversations)\n",
    "dataset = dataset.map(format_conversation)\n",
    "\n",
    "print(\"‚úÖ Dataset format√©\")\n",
    "print(f\"üìä Colonnes: {dataset.column_names}\")\n",
    "print(f\"üìè Taille: {len(dataset)} exemples\")\n",
    "\n",
    "# Exemple de texte format√©\n",
    "print(f\"\\nüìù Exemple de texte format√©:\")\n",
    "print(dataset[0][\"text\"])\n",
    "\n",
    "# Diviser en train/validation (80/20)\n",
    "dataset_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = dataset_split[\"train\"]\n",
    "eval_dataset = dataset_split[\"test\"]\n",
    "\n",
    "print(f\"\\nüìä Split des donn√©es:\")\n",
    "print(f\"  Train: {len(train_dataset)} exemples\")\n",
    "print(f\"  Validation: {len(eval_dataset)} exemples\")\n",
    "\n",
    "# Fonction de tokenisation\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenise les textes pour l'entra√Ænement\"\"\"\n",
    "    # Tokeniser avec padding et truncation\n",
    "    result = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # Pour l'entra√Ænement causal, les labels sont les m√™mes que input_ids\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Tokeniser les datasets\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"input\", \"output\", \"text\"]\n",
    ")\n",
    "\n",
    "eval_dataset = eval_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"input\", \"output\", \"text\"]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Datasets tokenis√©s\")\n",
    "print(f\"üîß Colonnes finales: {train_dataset.column_names}\")\n",
    "\n",
    "# V√©rifier la forme des donn√©es\n",
    "print(f\"üìê Forme d'un √©chantillon:\")\n",
    "sample = train_dataset[0]\n",
    "for key, value in sample.items():\n",
    "    print(f\"  {key}: {len(value) if isinstance(value, list) else type(value)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6ace19",
   "metadata": {},
   "source": [
    "## 4. ‚öôÔ∏è Configurer le Fine-tuning avec LoRA\n",
    "\n",
    "LoRA (Low-Rank Adaptation) est une technique qui permet de fine-tuner efficacement de gros mod√®les en n'entra√Ænant qu'une petite fraction des param√®tres. Cela r√©duit consid√©rablement les besoins en m√©moire et en calcul.\n",
    "\n",
    "**Avantages de LoRA :**\n",
    "- üöÄ Entra√Ænement plus rapide\n",
    "- üíæ Moins d'utilisation m√©moire\n",
    "- üéØ R√©sultats comparables au fine-tuning complet\n",
    "- üíΩ Mod√®les plus petits √† stocker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c86c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,  # Type de t√¢che: mod√®le de langage causal\n",
    "    r=16,                          # Rang de la d√©composition (plus bas = moins de param√®tres)\n",
    "    lora_alpha=32,                 # Param√®tre de scaling LoRA\n",
    "    lora_dropout=0.1,              # Dropout pour la r√©gularisation\n",
    "    target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"],  # Modules √† adapter (sp√©cifique au mod√®le)\n",
    "    bias=\"none\"                    # Ne pas adapter les bias\n",
    ")\n",
    "\n",
    "print(\"üîß Configuration LoRA:\")\n",
    "print(f\"  Rang (r): {lora_config.r}\")\n",
    "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  Dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"  Modules cibles: {lora_config.target_modules}\")\n",
    "\n",
    "# Appliquer LoRA au mod√®le\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Afficher les informations apr√®s LoRA\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nüìä Param√®tres apr√®s LoRA:\")\n",
    "print(f\"  Total: {total_params:,}\")\n",
    "print(f\"  Entra√Ænables: {trainable_params:,}\")\n",
    "print(f\"  Pourcentage entra√Ænable: {(trainable_params/total_params)*100:.2f}%\")\n",
    "print(f\"  R√©duction: {((total_params-trainable_params)/total_params)*100:.1f}% de param√®tres en moins √† entra√Æner!\")\n",
    "\n",
    "# V√©rifier que LoRA est bien appliqu√©\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f99744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration des param√®tres d'entra√Ænement\n",
    "output_dir = \"../models/finetuned\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    \n",
    "    # Param√®tres d'entra√Ænement\n",
    "    num_train_epochs=3,                    # Nombre d'√©poques\n",
    "    per_device_train_batch_size=2,         # Taille de batch (ajustez selon votre GPU)\n",
    "    per_device_eval_batch_size=2,          # Taille de batch pour l'√©valuation\n",
    "    gradient_accumulation_steps=4,         # Accumulation de gradients\n",
    "    \n",
    "    # Optimiseur\n",
    "    learning_rate=2e-4,                    # Taux d'apprentissage\n",
    "    weight_decay=0.01,                     # D√©croissance des poids\n",
    "    warmup_ratio=0.1,                      # Warmup\n",
    "    lr_scheduler_type=\"cosine\",            # Type de scheduler\n",
    "    \n",
    "    # Sauvegarde et √©valuation\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,                         # Sauvegarder tous les 50 steps\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=25,                         # √âvaluer tous les 25 steps\n",
    "    save_total_limit=3,                    # Garder seulement 3 checkpoints\n",
    "    load_best_model_at_end=True,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=10,\n",
    "    logging_dir=\"../logs\",\n",
    "    \n",
    "    # Optimisations\n",
    "    fp16=torch.cuda.is_available(),        # Pr√©cision mixte si GPU disponible\n",
    "    dataloader_num_workers=2,\n",
    "    remove_unused_columns=False,\n",
    "    \n",
    "    # Autres\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    "    report_to=[],                          # D√©sactiver W&B pour la d√©mo\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Arguments d'entra√Ænement configur√©s\")\n",
    "print(f\"üéØ √âpoques: {training_args.num_train_epochs}\")\n",
    "print(f\"üì¶ Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"üéöÔ∏è Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"üíæ R√©pertoire de sortie: {training_args.output_dir}\")\n",
    "print(f\"üî• FP16: {training_args.fp16}\")\n",
    "\n",
    "# Data collator pour l'entra√Ænement de mod√®les de langage\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Pas de masquage pour les mod√®les causaux\n",
    "    pad_to_multiple_of=8 if training_args.fp16 else None\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Data collator configur√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52cc512",
   "metadata": {},
   "source": [
    "## 5. üöÄ Lancer le Fine-tuning\n",
    "\n",
    "Maintenant que tout est configur√©, nous pouvons lancer l'entra√Ænement ! Le processus peut prendre quelques minutes selon votre mat√©riel.\n",
    "\n",
    "**‚è±Ô∏è Temps estim√© :**\n",
    "- CPU: 10-30 minutes\n",
    "- GPU (GTX 1060/RTX 2060): 5-15 minutes  \n",
    "- GPU (RTX 3080+): 2-5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c1ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er le trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer cr√©√©\")\n",
    "print(\"\\nüî• D√©but de l'entra√Ænement...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Lancer l'entra√Ænement\n",
    "start_time = datetime.now()\n",
    "\n",
    "try:\n",
    "    # Entra√Æner le mod√®le\n",
    "    trainer.train()\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"üéâ Entra√Ænement termin√© avec succ√®s !\")\n",
    "    print(f\"‚è±Ô∏è Temps total: {training_time}\")\n",
    "    \n",
    "    # Afficher les m√©triques finales\n",
    "    final_metrics = trainer.state.log_history[-1]\n",
    "    print(f\"\\nüìä M√©triques finales:\")\n",
    "    for key, value in final_metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur durante l'entra√Ænement: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e459ea6",
   "metadata": {},
   "source": [
    "## 6. üìà √âvaluer le Mod√®le Fine-tun√©\n",
    "\n",
    "Maintenant que l'entra√Ænement est termin√©, √©valuons les performances du mod√®le et testons-le avec quelques exemples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fa8ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluation sur le jeu de validation\n",
    "print(\"üîç √âvaluation sur le jeu de validation...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"üìä R√©sultats d'√©valuation:\")\n",
    "for metric, value in eval_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {metric}: {value}\")\n",
    "\n",
    "# Fonction pour g√©n√©rer du texte avec le mod√®le fine-tun√©\n",
    "def generate_response(prompt, max_length=200, temperature=0.7):\n",
    "    \"\"\"G√©n√®re une r√©ponse avec le mod√®le fine-tun√©\"\"\"\n",
    "    # Formater le prompt\n",
    "    formatted_prompt = f\"<|user|>{prompt}<|endoftext|><|assistant|>\"\n",
    "    \n",
    "    # Tokeniser\n",
    "    inputs = tokenizer.encode(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # G√©n√©rer\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=len(inputs[0]) + max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # D√©coder seulement la partie g√©n√©r√©e\n",
    "    generated_text = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
    "    \n",
    "    # Nettoyer la r√©ponse\n",
    "    if \"<|endoftext|>\" in generated_text:\n",
    "        generated_text = generated_text.split(\"<|endoftext|>\")[0]\n",
    "    \n",
    "    return generated_text.strip()\n",
    "\n",
    "# Tester le mod√®le avec quelques exemples\n",
    "test_prompts = [\n",
    "    \"Bonjour, pouvez-vous m'aider ?\",\n",
    "    \"Qu'est-ce que le machine learning ?\",\n",
    "    \"Comment puis-je apprendre l'IA ?\",\n",
    "    \"Expliquez-moi Python simplement\",\n",
    "    \"Merci pour votre aide !\"\n",
    "]\n",
    "\n",
    "print(\"\\nüß™ Test du mod√®le fine-tun√©:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\nüìù Test {i}:\")\n",
    "    print(f\"üë§ Utilisateur: {prompt}\")\n",
    "    \n",
    "    response = generate_response(prompt)\n",
    "    print(f\"ü§ñ Assistant: {response}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da264bc0",
   "metadata": {},
   "source": [
    "## 7. üíæ Sauvegarder et Charger le Mod√®le Fine-tun√©\n",
    "\n",
    "Une fois satisfait des r√©sultats, nous pouvons sauvegarder le mod√®le pour une utilisation future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b22cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le mod√®le fine-tun√©\n",
    "save_path = \"../models/my_finetuned_model\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "print(f\"üíæ Sauvegarde du mod√®le dans {save_path}...\")\n",
    "\n",
    "# Sauvegarder le mod√®le LoRA\n",
    "model.save_pretrained(save_path)\n",
    "\n",
    "# Sauvegarder le tokenizer\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "# Sauvegarder les m√©triques d'entra√Ænement\n",
    "metrics_path = os.path.join(save_path, \"training_metrics.json\")\n",
    "with open(metrics_path, \"w\") as f:\n",
    "    json.dump(trainer.state.log_history, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Mod√®le sauvegard√© avec succ√®s !\")\n",
    "\n",
    "# Lister les fichiers sauvegard√©s\n",
    "import os\n",
    "saved_files = os.listdir(save_path)\n",
    "print(f\"\\nüìÅ Fichiers sauvegard√©s:\")\n",
    "for file in saved_files:\n",
    "    file_path = os.path.join(save_path, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        size = os.path.getsize(file_path) / (1024*1024)  # MB\n",
    "        print(f\"  üìÑ {file} ({size:.1f} MB)\")\n",
    "\n",
    "print(f\"\\nüìä Taille totale du mod√®le:\")\n",
    "total_size = sum(os.path.getsize(os.path.join(save_path, f)) \n",
    "                for f in saved_files if os.path.isfile(os.path.join(save_path, f)))\n",
    "print(f\"  {total_size / (1024*1024):.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e3bdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©monstration du rechargement du mod√®le\n",
    "print(\"üîÑ D√©monstration du rechargement du mod√®le...\")\n",
    "\n",
    "# Pour recharger le mod√®le plus tard, utilisez ce code:\n",
    "from peft import PeftModel\n",
    "\n",
    "def load_finetuned_model(model_path, base_model_name):\n",
    "    \"\"\"Charge un mod√®le fine-tun√© avec LoRA\"\"\"\n",
    "    \n",
    "    # Charger le tokenizer\n",
    "    loaded_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    # Charger le mod√®le de base\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "    )\n",
    "    \n",
    "    # Charger les adaptateurs LoRA\n",
    "    loaded_model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    \n",
    "    return loaded_model, loaded_tokenizer\n",
    "\n",
    "# Exemple d'utilisation (comment√© pour √©viter de recharger maintenant)\n",
    "\"\"\"\n",
    "loaded_model, loaded_tokenizer = load_finetuned_model(\n",
    "    save_path, \n",
    "    MODEL_NAME\n",
    ")\n",
    "print(\"‚úÖ Mod√®le recharg√© avec succ√®s !\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìù Code de rechargement pr√™t √† utiliser !\")\n",
    "\n",
    "# Afficher le code d'utilisation\n",
    "usage_code = f'''\n",
    "# Pour utiliser votre mod√®le fine-tun√© plus tard:\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Chemins\n",
    "model_path = \"{save_path}\"\n",
    "base_model_name = \"{MODEL_NAME}\"\n",
    "\n",
    "# Charger le tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Charger le mod√®le de base\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Charger les adaptateurs LoRA\n",
    "model = PeftModel.from_pretrained(base_model, model_path)\n",
    "\n",
    "# Fonction de g√©n√©ration\n",
    "def chat_with_model(prompt):\n",
    "    formatted_prompt = f\"<|user|>{{prompt}}<|endoftext|><|assistant|>\"\n",
    "    inputs = tokenizer.encode(formatted_prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=len(inputs[0]) + 200,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
    "    return response.split(\"<|endoftext|>\")[0].strip()\n",
    "\n",
    "# Utilisation\n",
    "response = chat_with_model(\"Bonjour, comment allez-vous ?\")\n",
    "print(response)\n",
    "'''\n",
    "\n",
    "print(\"üí° Code d'utilisation:\")\n",
    "print(usage_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea6fd8c",
   "metadata": {},
   "source": [
    "## üéâ Conclusion\n",
    "\n",
    "F√©licitations ! Vous avez r√©ussi √† fine-tuner un LLM en local avec LoRA. \n",
    "\n",
    "### üéØ Ce que vous avez appris :\n",
    "- ‚úÖ Charger et pr√©parer un mod√®le LLM open-source\n",
    "- ‚úÖ Pr√©parer et formater des donn√©es d'entra√Ænement\n",
    "- ‚úÖ Configurer LoRA pour un fine-tuning efficace\n",
    "- ‚úÖ Entra√Æner le mod√®le avec des param√®tres optimaux\n",
    "- ‚úÖ √âvaluer et tester le mod√®le fine-tun√©\n",
    "- ‚úÖ Sauvegarder et recharger le mod√®le\n",
    "\n",
    "### üöÄ Prochaines √©tapes :\n",
    "1. **Donn√©es r√©elles** : Remplacez les donn√©es d'exemple par vos propres donn√©es\n",
    "2. **Mod√®les plus gros** : Essayez des mod√®les plus grands comme Llama 2 7B\n",
    "3. **Optimisations** : Exp√©rimentez avec diff√©rents param√®tres LoRA\n",
    "4. **D√©ploiement** : Int√©grez votre mod√®le dans une application\n",
    "5. **√âvaluation avanc√©e** : Utilisez des m√©triques plus sophistiqu√©es\n",
    "\n",
    "### üìö Ressources utiles :\n",
    "- [Documentation PEFT](https://huggingface.co/docs/peft/)\n",
    "- [LoRA Paper](https://arxiv.org/abs/2106.09685)\n",
    "- [Transformers Documentation](https://huggingface.co/docs/transformers/)\n",
    "- [Datasets Documentation](https://huggingface.co/docs/datasets/)\n",
    "\n",
    "### üí° Conseils pour l'optimisation :\n",
    "- **Augmentez les donn√©es** : Plus de donn√©es = meilleur mod√®le\n",
    "- **Ajustez LoRA** : Exp√©rimentez avec `r` et `alpha`\n",
    "- **Monitoring** : Utilisez W&B ou TensorBoard pour suivre l'entra√Ænement\n",
    "- **GPU** : Un GPU acc√©l√®re consid√©rablement le processus\n",
    "\n",
    "Bonne chance avec vos projets de fine-tuning ! üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
